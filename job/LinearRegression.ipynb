{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"pantheon\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_dato=1811253, data_ora=datetime.datetime(2018, 10, 12, 14, 25, 15), temp1_media=25.99, temp1_min=25.93, temp1_max=26.08, ur1_media=56.69, ur1_min=52.56, ur1_max=58.97, temp1_ur1_n_letture=5, pioggia_mm=0.0, rad W/mq=75.0, rad_n_letture=5, wind_dir=6, wind_dir_n_letture=5, wind_speed_media=0.3, wind_speed_max=15.62, wind_speed_n_letture=25, pressione_mbar=1085.94, pressione_standard_mbar=1118.68, pressione_n_letture=5, rad W/mq array='{}')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf = spark.read.format('com.databricks.spark.csv')\\\n",
    ".options(delimiter=';',header='true', inferschema='true').load('input/pantheon20190612-stazione.csv')\n",
    "stazioneDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_dato: integer (nullable = true)\n",
      " |-- data_ora: timestamp (nullable = true)\n",
      " |-- temp1_media: double (nullable = true)\n",
      " |-- temp1_min: double (nullable = true)\n",
      " |-- temp1_max: double (nullable = true)\n",
      " |-- ur1_media: double (nullable = true)\n",
      " |-- ur1_min: double (nullable = true)\n",
      " |-- ur1_max: double (nullable = true)\n",
      " |-- temp1_ur1_n_letture: integer (nullable = true)\n",
      " |-- pioggia_mm: double (nullable = true)\n",
      " |-- rad W/mq: double (nullable = true)\n",
      " |-- rad_n_letture: integer (nullable = true)\n",
      " |-- wind_dir: integer (nullable = true)\n",
      " |-- wind_dir_n_letture: integer (nullable = true)\n",
      " |-- wind_speed_media: double (nullable = true)\n",
      " |-- wind_speed_max: double (nullable = true)\n",
      " |-- wind_speed_n_letture: integer (nullable = true)\n",
      " |-- pressione_mbar: double (nullable = true)\n",
      " |-- pressione_standard_mbar: double (nullable = true)\n",
      " |-- pressione_n_letture: integer (nullable = true)\n",
      " |-- rad W/mq array: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stazioneDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questa cella serve ad individuare i blocchi del dataframe accomunati da una caratteristica comune, ad esempio in questo caso\n",
    "# vogliamo raggruppare tutti i blocchi con radiazione nulla e tutti quelli con radiazione non nulla\n",
    "# la funzione lag serve , data una certa riga , a selezionare la riga che si trova ad un certo offset da questa, nel nostro caso 1\n",
    "# usiamo lag per selezionare la riga subito sotto alla riga di inizio di un blocco \n",
    "# coalesce serve per selezionare, data una lista di colonne, la prima colonna non nulla\n",
    "# count considera nel conteggio solo le celle non vuote, così può essere utile per individuare una sottocolonna con tutti valori non nulli\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "nullRadSlot = stazioneDf\n",
    "nullRadSlot = nullRadSlot.withColumn(\"isnull\", F.when(nullRadSlot['rad W/mq'].isNull(), True).otherwise(False))\n",
    "nullRadSlot = nullRadSlot.withColumn(\"lag_isnull\", F.lag(nullRadSlot[\"isnull\"],1).over(Window.orderBy(nullRadSlot[\"data_ora\"])))\n",
    "nullRadSlot = nullRadSlot.withColumn(\"change\", F.coalesce(nullRadSlot[\"isnull\"]!=nullRadSlot[\"lag_isnull\"],F.lit(False)))\n",
    "nullRadSlot = nullRadSlot.withColumn(\"block\", F.sum(nullRadSlot[\"change\"].cast(\"int\")).over(Window.orderBy(nullRadSlot[\"data_ora\"])))\\\n",
    "  .groupBy(\"block\")\\\n",
    "  .agg(F.min(nullRadSlot[\"data_ora\"]).alias('mindata'),\n",
    "    F.max(nullRadSlot[\"data_ora\"]).alias('maxdata'),\n",
    "    (F.count(nullRadSlot['rad W/mq'])==0).alias('blocco_isnull'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+-------------+\n",
      "|block|            mindata|            maxdata|blocco_isnull|\n",
      "+-----+-------------------+-------------------+-------------+\n",
      "|    0|2018-10-12 14:25:15|2019-05-06 08:55:28|        false|\n",
      "|    1|2019-05-06 13:40:10|2019-06-12 09:25:15|         true|\n",
      "+-----+-------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nullRadSlot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeFmt = \"yyyy-MM-dd'T'HH:mm:ss.SSS\"\n",
    "timeDiff = (F.unix_timestamp('maxdata', format=timeFmt)\n",
    "            - F.unix_timestamp('mindata', format=timeFmt))\n",
    "nullRadSlot = nullRadSlot.withColumn(\"Duration\", timeDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+-------------+--------+\n",
      "|block|            mindata|            maxdata|blocco_isnull|Duration|\n",
      "+-----+-------------------+-------------------+-------------+--------+\n",
      "|    0|2018-10-12 14:25:15|2019-05-06 08:55:28|        false|17778613|\n",
      "|    1|2019-05-06 13:40:10|2019-06-12 09:25:15|         true| 3181505|\n",
      "+-----+-------------------+-------------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nullRadSlot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifica dei valori nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.id_dato.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.temp1_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.ur1_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.wind_speed_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20206"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf['rad W/mq'].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo valori medi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        F.avg(c).alias(c) for c in df.columns if c in include\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioneDf = fill_with_mean(stazioneDf, ['wind_speed_media'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.wind_speed_media.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.b per esprimere più condizioni in and o or bisogna racchiudere le condizioni tra parentesi\n",
    "from pyspark.sql.functions import hour\n",
    "avgT=stazioneDf.filter((stazioneDf['rad W/mq'].isNotNull())& \\\n",
    "                   ((hour(stazioneDf['data_ora'])<=18)|(hour(stazioneDf['data_ora'])>5)))\n",
    "avgT=stazioneDf.filter((stazioneDf['rad W/mq'].isNotNull())& \\\n",
    "                   ((hour(stazioneDf['data_ora'])<=18)|(hour(stazioneDf['data_ora'])>5)))\n",
    "from pyspark.sql.functions import mean\n",
    "stats=avgT.select([mean('rad W/mq')]).first()\n",
    "avgRadValue= stats.asDict()\n",
    "avgRadValue = avgRadValue['avg(rad W/mq)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.87738295555782"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRadValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,hour\n",
    "\n",
    "stazioneDf = stazioneDf.withColumn('rad W/mq',when((stazioneDf['rad W/mq'].isNull())& ((hour(stazioneDf['data_ora'])>18)|\\\n",
    "                                               (hour(stazioneDf['data_ora'])<=5)),0)\\\n",
    "                               .when((stazioneDf['rad W/mq'].isNull())& ((hour(stazioneDf['data_ora'])<=18)|\\\n",
    "                                               (hour(stazioneDf['data_ora'])>5)),avgRadValue)\\\n",
    "                               .otherwise(stazioneDf['rad W/mq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter((stazioneDf['rad W/mq'].isNull())).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con questa opzione della Spark Session impostata a True, non c'è più bisogno di usare show per vedere i dataframe;\n",
    "# inoltre li posso vedere in modo non sballato\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>rad W/mq</th><th>data_ora</th></tr>\n",
       "<tr><td>75.0</td><td>2018-10-12 14:25:15</td></tr>\n",
       "<tr><td>104.1</td><td>2018-10-12 14:30:12</td></tr>\n",
       "<tr><td>234.4</td><td>2018-10-12 14:35:11</td></tr>\n",
       "<tr><td>247.2</td><td>2018-10-12 14:40:13</td></tr>\n",
       "<tr><td>206.6</td><td>2018-10-12 14:45:13</td></tr>\n",
       "<tr><td>250.3</td><td>2018-10-12 14:50:10</td></tr>\n",
       "<tr><td>307.5</td><td>2018-10-12 14:55:13</td></tr>\n",
       "<tr><td>360.5</td><td>2018-10-12 15:00:15</td></tr>\n",
       "<tr><td>307.5</td><td>2018-10-12 15:05:11</td></tr>\n",
       "<tr><td>388.4</td><td>2018-10-12 15:10:13</td></tr>\n",
       "<tr><td>367.5</td><td>2018-10-12 15:15:13</td></tr>\n",
       "<tr><td>348.9</td><td>2018-10-12 15:20:10</td></tr>\n",
       "<tr><td>340.4</td><td>2018-10-12 15:25:13</td></tr>\n",
       "<tr><td>321.8</td><td>2018-10-12 15:30:15</td></tr>\n",
       "<tr><td>304.4</td><td>2018-10-12 15:35:11</td></tr>\n",
       "<tr><td>294.4</td><td>2018-10-12 15:40:13</td></tr>\n",
       "<tr><td>277.0</td><td>2018-10-12 15:45:13</td></tr>\n",
       "<tr><td>259.8</td><td>2018-10-12 15:50:12</td></tr>\n",
       "<tr><td>231.3</td><td>2018-10-12 16:00:13</td></tr>\n",
       "<tr><td>195.2</td><td>2018-10-12 16:15:10</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+-------------------+\n",
       "|rad W/mq|           data_ora|\n",
       "+--------+-------------------+\n",
       "|    75.0|2018-10-12 14:25:15|\n",
       "|   104.1|2018-10-12 14:30:12|\n",
       "|   234.4|2018-10-12 14:35:11|\n",
       "|   247.2|2018-10-12 14:40:13|\n",
       "|   206.6|2018-10-12 14:45:13|\n",
       "|   250.3|2018-10-12 14:50:10|\n",
       "|   307.5|2018-10-12 14:55:13|\n",
       "|   360.5|2018-10-12 15:00:15|\n",
       "|   307.5|2018-10-12 15:05:11|\n",
       "|   388.4|2018-10-12 15:10:13|\n",
       "|   367.5|2018-10-12 15:15:13|\n",
       "|   348.9|2018-10-12 15:20:10|\n",
       "|   340.4|2018-10-12 15:25:13|\n",
       "|   321.8|2018-10-12 15:30:15|\n",
       "|   304.4|2018-10-12 15:35:11|\n",
       "|   294.4|2018-10-12 15:40:13|\n",
       "|   277.0|2018-10-12 15:45:13|\n",
       "|   259.8|2018-10-12 15:50:12|\n",
       "|   231.3|2018-10-12 16:00:13|\n",
       "|   195.2|2018-10-12 16:15:10|\n",
       "+--------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.select(stazioneDf['rad W/mq'],stazioneDf['data_ora'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggiunta della colonna evapotraspirazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Et0 = 0.0393 Rs * sqrt(T+9.5)-0.19* Rs^0.6 * lat^0.15 +0.048 *(T+20)(1-UMI/100)* u2^0.7\n",
    "LATITUDE=0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,hour\n",
    "from pyspark.sql.functions import sum,avg \n",
    "# con questa operazione mi calcolo un dataframe in cui ho per ogni giorno la temperatura minima e massima\n",
    "maxMinTemperaterByDay = stazioneDf.select(to_date(stazioneDf['data_ora']).alias('giorno'),stazioneDf['temp1_min'],stazioneDf['temp1_max'])\\\n",
    "    .groupBy('giorno').agg({\"temp1_max\":\"max\",\"temp1_min\":\"min\"})\\\n",
    "    .withColumnRenamed(\"max(temp1_max)\",\"t_max\").withColumnRenamed(\"min(temp1_min)\",\"t_min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questa cella serve aggregare i valori per ora, in base da avere valori più realistici di evapotraspirazione\n",
    "eachHourDf = stazioneDf.select([\"data_ora\",\"wind_speed_media\",\"temp1_media\",\"rad W/mq\",\"ur1_media\"])\\\n",
    ".groupBy(to_date(stazioneDf['data_ora']).alias(\"data\"),hour(stazioneDf['data_ora'])).agg({\"wind_speed_media\":\"avg\",\"rad W/mq\":\"avg\",\"ur1_media\":\"avg\"\\\n",
    "                                               ,\"temp1_media\":\"avg\"}).\\\n",
    "    withColumnRenamed('avg(rad W/mq)','Rs').withColumnRenamed('avg(temp1_media)','T')\\\n",
    "    .withColumnRenamed('avg(ur1_media)','RH').withColumnRenamed('avg(wind_speed_media)','u2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>data</th><th>hour(data_ora)</th><th>RH</th><th>Rs</th><th>u2</th><th>T</th><th>giorno</th><th>t_max</th><th>t_min</th></tr>\n",
       "<tr><td>2018-10-17</td><td>18</td><td>90.60000000000001</td><td>0.3</td><td>4.27066381798</td><td>17.1825</td><td>2018-10-17</td><td>23.01</td><td>12.09</td></tr>\n",
       "<tr><td>2018-10-18</td><td>18</td><td>89.125</td><td>0.45</td><td>5.654218423973333</td><td>16.18</td><td>2018-10-18</td><td>27.01</td><td>12.3</td></tr>\n",
       "<tr><td>2018-10-20</td><td>20</td><td>78.815</td><td>0.5</td><td>1.9810546059933332</td><td>13.4375</td><td>2018-10-20</td><td>25.51</td><td>8.98</td></tr>\n",
       "<tr><td>2018-10-29</td><td>4</td><td>86.75999999999999</td><td>0.65</td><td>7.67</td><td>16.830000000000002</td><td>2018-10-29</td><td>21.19</td><td>9.98</td></tr>\n",
       "<tr><td>2018-11-01</td><td>18</td><td>94.44250000000001</td><td>0.44999999999999996</td><td>0.6025</td><td>14.47</td><td>2018-11-01</td><td>16.07</td><td>11.63</td></tr>\n",
       "<tr><td>2018-11-07</td><td>12</td><td>79.975</td><td>168.425</td><td>5.06</td><td>15.43</td><td>2018-11-07</td><td>18.24</td><td>5.66</td></tr>\n",
       "<tr><td>2018-11-08</td><td>11</td><td>76.39</td><td>469.75</td><td>2.9975</td><td>16.08</td><td>2018-11-08</td><td>19.08</td><td>6.17</td></tr>\n",
       "<tr><td>2018-11-11</td><td>3</td><td>96.07</td><td>0.95</td><td>5.654218423973333</td><td>5.8275</td><td>2018-11-11</td><td>19.62</td><td>3.99</td></tr>\n",
       "<tr><td>2018-11-27</td><td>11</td><td>82.28</td><td>87.1</td><td>7.5625</td><td>9.447499999999998</td><td>2018-11-27</td><td>10.89</td><td>7.14</td></tr>\n",
       "<tr><td>2018-12-09</td><td>22</td><td>75.84</td><td>0.9500000000000001</td><td>4.8774999999999995</td><td>10.955</td><td>2018-12-09</td><td>15.62</td><td>-0.49</td></tr>\n",
       "<tr><td>2018-12-10</td><td>7</td><td>94.89999999999999</td><td>2.5749999999999997</td><td>3.217109211986666</td><td>2.3475</td><td>2018-12-10</td><td>15.44</td><td>1.66</td></tr>\n",
       "<tr><td>2018-12-19</td><td>12</td><td>55.5</td><td>404.775</td><td>1.24</td><td>8.575</td><td>2018-12-19</td><td>9.92</td><td>-4.11</td></tr>\n",
       "<tr><td>2019-01-16</td><td>12</td><td>65.395</td><td>344.15000000000003</td><td>8.434999999999999</td><td>10.9725</td><td>2019-01-16</td><td>12.81</td><td>-2.34</td></tr>\n",
       "<tr><td>2019-02-07</td><td>21</td><td>89.0425</td><td>0.7000000000000001</td><td>0.61</td><td>3.5075000000000003</td><td>2019-02-07</td><td>15.54</td><td>-1.4</td></tr>\n",
       "<tr><td>2019-02-16</td><td>11</td><td>44.7625</td><td>533.925</td><td>5.4799999999999995</td><td>11.309999999999999</td><td>2019-02-16</td><td>15.46</td><td>-0.92</td></tr>\n",
       "<tr><td>2019-02-21</td><td>12</td><td>56.857499999999995</td><td>570.125</td><td>5.242500000000001</td><td>13.105</td><td>2019-02-21</td><td>15.96</td><td>-1.32</td></tr>\n",
       "<tr><td>2019-03-12</td><td>14</td><td>20.810000000000002</td><td>622.1</td><td>11.015</td><td>13.620000000000001</td><td>2019-03-12</td><td>14.66</td><td>0.84</td></tr>\n",
       "<tr><td>2019-03-17</td><td>2</td><td>94.05</td><td>0.65</td><td>5.654218423973333</td><td>6.8999999999999995</td><td>2019-03-17</td><td>15.59</td><td>5.3</td></tr>\n",
       "<tr><td>2019-03-28</td><td>5</td><td>66.605</td><td>0.4</td><td>6.3575</td><td>5.74</td><td>2019-03-28</td><td>17.41</td><td>5.57</td></tr>\n",
       "<tr><td>2019-04-18</td><td>22</td><td>82.49</td><td>0.4666666666666666</td><td>2.9447394746577777</td><td>8.916666666666666</td><td>2019-04-18</td><td>23.28</td><td>5.07</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+--------------+------------------+-------------------+------------------+------------------+----------+-----+-----+\n",
       "|      data|hour(data_ora)|                RH|                 Rs|                u2|                 T|    giorno|t_max|t_min|\n",
       "+----------+--------------+------------------+-------------------+------------------+------------------+----------+-----+-----+\n",
       "|2018-10-17|            18| 90.60000000000001|                0.3|     4.27066381798|           17.1825|2018-10-17|23.01|12.09|\n",
       "|2018-10-18|            18|            89.125|               0.45| 5.654218423973333|             16.18|2018-10-18|27.01| 12.3|\n",
       "|2018-10-20|            20|            78.815|                0.5|1.9810546059933332|           13.4375|2018-10-20|25.51| 8.98|\n",
       "|2018-10-29|             4| 86.75999999999999|               0.65|              7.67|16.830000000000002|2018-10-29|21.19| 9.98|\n",
       "|2018-11-01|            18| 94.44250000000001|0.44999999999999996|            0.6025|             14.47|2018-11-01|16.07|11.63|\n",
       "|2018-11-07|            12|            79.975|            168.425|              5.06|             15.43|2018-11-07|18.24| 5.66|\n",
       "|2018-11-08|            11|             76.39|             469.75|            2.9975|             16.08|2018-11-08|19.08| 6.17|\n",
       "|2018-11-11|             3|             96.07|               0.95| 5.654218423973333|            5.8275|2018-11-11|19.62| 3.99|\n",
       "|2018-11-27|            11|             82.28|               87.1|            7.5625| 9.447499999999998|2018-11-27|10.89| 7.14|\n",
       "|2018-12-09|            22|             75.84| 0.9500000000000001|4.8774999999999995|            10.955|2018-12-09|15.62|-0.49|\n",
       "|2018-12-10|             7| 94.89999999999999| 2.5749999999999997| 3.217109211986666|            2.3475|2018-12-10|15.44| 1.66|\n",
       "|2018-12-19|            12|              55.5|            404.775|              1.24|             8.575|2018-12-19| 9.92|-4.11|\n",
       "|2019-01-16|            12|            65.395| 344.15000000000003| 8.434999999999999|           10.9725|2019-01-16|12.81|-2.34|\n",
       "|2019-02-07|            21|           89.0425| 0.7000000000000001|              0.61|3.5075000000000003|2019-02-07|15.54| -1.4|\n",
       "|2019-02-16|            11|           44.7625|            533.925|5.4799999999999995|11.309999999999999|2019-02-16|15.46|-0.92|\n",
       "|2019-02-21|            12|56.857499999999995|            570.125| 5.242500000000001|            13.105|2019-02-21|15.96|-1.32|\n",
       "|2019-03-12|            14|20.810000000000002|              622.1|            11.015|13.620000000000001|2019-03-12|14.66| 0.84|\n",
       "|2019-03-17|             2|             94.05|               0.65| 5.654218423973333|6.8999999999999995|2019-03-17|15.59|  5.3|\n",
       "|2019-03-28|             5|            66.605|                0.4|            6.3575|              5.74|2019-03-28|17.41| 5.57|\n",
       "|2019-04-18|            22|             82.49| 0.4666666666666666|2.9447394746577777| 8.916666666666666|2019-04-18|23.28| 5.07|\n",
       "+----------+--------------+------------------+-------------------+------------------+------------------+----------+-----+-----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join tra tabella oraria e tabella delle temperature minime e massime, per avere per ogni record della prima\n",
    "# tabella la temperatura giornaliera massima e minima, utile per il calcolo di Et0\n",
    "eachHourDf.join(maxMinTemperaterByDay,eachHourDf.data==maxMinTemperaterByDay.giorno,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>data</th><th>hour(data_ora)</th><th>RH</th><th>Rs</th><th>u2</th><th>T</th></tr>\n",
       "<tr><td>2018-10-17</td><td>18</td><td>90.60000000000001</td><td>0.3</td><td>4.27066381798</td><td>17.1825</td></tr>\n",
       "<tr><td>2018-10-18</td><td>18</td><td>89.125</td><td>0.45</td><td>5.654218423973333</td><td>16.18</td></tr>\n",
       "<tr><td>2018-10-20</td><td>20</td><td>78.815</td><td>0.5</td><td>1.9810546059933332</td><td>13.4375</td></tr>\n",
       "<tr><td>2018-10-29</td><td>4</td><td>86.75999999999999</td><td>0.65</td><td>7.67</td><td>16.830000000000002</td></tr>\n",
       "<tr><td>2018-11-01</td><td>18</td><td>94.44250000000001</td><td>0.44999999999999996</td><td>0.6025</td><td>14.47</td></tr>\n",
       "<tr><td>2018-11-07</td><td>12</td><td>79.975</td><td>168.425</td><td>5.06</td><td>15.43</td></tr>\n",
       "<tr><td>2018-11-08</td><td>11</td><td>76.39</td><td>469.75</td><td>2.9975</td><td>16.08</td></tr>\n",
       "<tr><td>2018-11-11</td><td>3</td><td>96.07</td><td>0.95</td><td>5.654218423973333</td><td>5.8275</td></tr>\n",
       "<tr><td>2018-11-27</td><td>11</td><td>82.28</td><td>87.1</td><td>7.5625</td><td>9.447499999999998</td></tr>\n",
       "<tr><td>2018-12-09</td><td>22</td><td>75.84</td><td>0.9500000000000001</td><td>4.8774999999999995</td><td>10.955</td></tr>\n",
       "<tr><td>2018-12-10</td><td>7</td><td>94.89999999999999</td><td>2.5749999999999997</td><td>3.217109211986666</td><td>2.3475</td></tr>\n",
       "<tr><td>2018-12-19</td><td>12</td><td>55.5</td><td>404.775</td><td>1.24</td><td>8.575</td></tr>\n",
       "<tr><td>2019-01-16</td><td>12</td><td>65.395</td><td>344.15000000000003</td><td>8.434999999999999</td><td>10.9725</td></tr>\n",
       "<tr><td>2019-02-07</td><td>21</td><td>89.0425</td><td>0.7000000000000001</td><td>0.61</td><td>3.5075000000000003</td></tr>\n",
       "<tr><td>2019-02-16</td><td>11</td><td>44.7625</td><td>533.925</td><td>5.4799999999999995</td><td>11.309999999999999</td></tr>\n",
       "<tr><td>2019-02-21</td><td>12</td><td>56.857499999999995</td><td>570.125</td><td>5.242500000000001</td><td>13.105</td></tr>\n",
       "<tr><td>2019-03-12</td><td>14</td><td>20.810000000000002</td><td>622.1</td><td>11.015</td><td>13.620000000000001</td></tr>\n",
       "<tr><td>2019-03-17</td><td>2</td><td>94.05</td><td>0.65</td><td>5.654218423973333</td><td>6.8999999999999995</td></tr>\n",
       "<tr><td>2019-03-28</td><td>5</td><td>66.605</td><td>0.4</td><td>6.3575</td><td>5.74</td></tr>\n",
       "<tr><td>2019-04-18</td><td>22</td><td>82.49</td><td>0.4666666666666666</td><td>2.9447394746577777</td><td>8.916666666666666</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "DataFrame[data: date, hour(data_ora): int, RH: double, Rs: double, u2: double, T: double]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachHourDf = eachHourDf.select('giorno','hour(data_ora)','RH','Rs','u2','T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>data</th><th>hour(data_ora)</th><th>RH</th><th>Rs</th><th>u2</th><th>T</th><th>delta</th></tr>\n",
       "<tr><td>2018-10-17</td><td>18</td><td>90.60000000000001</td><td>0.3</td><td>4.27066381798</td><td>17.1825</td><td>1.0872826588951532</td></tr>\n",
       "<tr><td>2018-10-18</td><td>18</td><td>89.125</td><td>0.45</td><td>5.654218423973333</td><td>16.18</td><td>1.0319605003978203</td></tr>\n",
       "<tr><td>2018-10-20</td><td>20</td><td>78.815</td><td>0.5</td><td>1.9810546059933332</td><td>13.4375</td><td>0.8758945993908427</td></tr>\n",
       "<tr><td>2018-10-29</td><td>4</td><td>86.75999999999999</td><td>0.65</td><td>7.67</td><td>16.830000000000002</td><td>1.0679334680651833</td></tr>\n",
       "<tr><td>2018-11-01</td><td>18</td><td>94.44250000000001</td><td>0.44999999999999996</td><td>0.6025</td><td>14.47</td><td>0.9354757316096938</td></tr>\n",
       "<tr><td>2018-11-07</td><td>12</td><td>79.975</td><td>168.425</td><td>5.06</td><td>15.43</td><td>0.9899751321433905</td></tr>\n",
       "<tr><td>2018-11-08</td><td>11</td><td>76.39</td><td>469.75</td><td>2.9975</td><td>16.08</td><td>1.0263921807940448</td></tr>\n",
       "<tr><td>2018-11-11</td><td>3</td><td>96.07</td><td>0.95</td><td>5.654218423973333</td><td>5.8275</td><td>0.40400437435599107</td></tr>\n",
       "<tr><td>2018-11-27</td><td>11</td><td>82.28</td><td>87.1</td><td>7.5625</td><td>9.447499999999998</td><td>0.6358919576004304</td></tr>\n",
       "<tr><td>2018-12-09</td><td>22</td><td>75.84</td><td>0.9500000000000001</td><td>4.8774999999999995</td><td>10.955</td><td>0.7284308451562326</td></tr>\n",
       "<tr><td>2018-12-10</td><td>7</td><td>94.89999999999999</td><td>2.5749999999999997</td><td>3.217109211986666</td><td>2.3475</td><td>0.16750653284067746</td></tr>\n",
       "<tr><td>2018-12-19</td><td>12</td><td>55.5</td><td>404.775</td><td>1.24</td><td>8.575</td><td>0.5812692341788984</td></tr>\n",
       "<tr><td>2019-01-16</td><td>12</td><td>65.395</td><td>344.15000000000003</td><td>8.434999999999999</td><td>10.9725</td><td>0.7294916223823898</td></tr>\n",
       "<tr><td>2019-02-07</td><td>21</td><td>89.0425</td><td>0.7000000000000001</td><td>0.61</td><td>3.5075000000000003</td><td>0.24787322414990423</td></tr>\n",
       "<tr><td>2019-02-16</td><td>11</td><td>44.7625</td><td>533.925</td><td>5.4799999999999995</td><td>11.309999999999999</td><td>0.7498896713561357</td></tr>\n",
       "<tr><td>2019-02-21</td><td>12</td><td>56.857499999999995</td><td>570.125</td><td>5.242500000000001</td><td>13.105</td><td>0.8564913602169713</td></tr>\n",
       "<tr><td>2019-03-12</td><td>14</td><td>20.810000000000002</td><td>622.1</td><td>11.015</td><td>13.620000000000001</td><td>0.8864995182148631</td></tr>\n",
       "<tr><td>2019-03-17</td><td>2</td><td>94.05</td><td>0.65</td><td>5.654218423973333</td><td>6.8999999999999995</td><td>0.4741652530350318</td></tr>\n",
       "<tr><td>2019-03-28</td><td>5</td><td>66.605</td><td>0.4</td><td>6.3575</td><td>5.74</td><td>0.39822482806114856</td></tr>\n",
       "<tr><td>2019-04-18</td><td>22</td><td>82.49</td><td>0.4666666666666666</td><td>2.9447394746577777</td><td>8.916666666666666</td><td>0.6027532917440291</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+--------------+------------------+-------------------+------------------+------------------+-------------------+\n",
       "|      data|hour(data_ora)|                RH|                 Rs|                u2|                 T|              delta|\n",
       "+----------+--------------+------------------+-------------------+------------------+------------------+-------------------+\n",
       "|2018-10-17|            18| 90.60000000000001|                0.3|     4.27066381798|           17.1825| 1.0872826588951532|\n",
       "|2018-10-18|            18|            89.125|               0.45| 5.654218423973333|             16.18| 1.0319605003978203|\n",
       "|2018-10-20|            20|            78.815|                0.5|1.9810546059933332|           13.4375| 0.8758945993908427|\n",
       "|2018-10-29|             4| 86.75999999999999|               0.65|              7.67|16.830000000000002| 1.0679334680651833|\n",
       "|2018-11-01|            18| 94.44250000000001|0.44999999999999996|            0.6025|             14.47| 0.9354757316096938|\n",
       "|2018-11-07|            12|            79.975|            168.425|              5.06|             15.43| 0.9899751321433905|\n",
       "|2018-11-08|            11|             76.39|             469.75|            2.9975|             16.08| 1.0263921807940448|\n",
       "|2018-11-11|             3|             96.07|               0.95| 5.654218423973333|            5.8275|0.40400437435599107|\n",
       "|2018-11-27|            11|             82.28|               87.1|            7.5625| 9.447499999999998| 0.6358919576004304|\n",
       "|2018-12-09|            22|             75.84| 0.9500000000000001|4.8774999999999995|            10.955| 0.7284308451562326|\n",
       "|2018-12-10|             7| 94.89999999999999| 2.5749999999999997| 3.217109211986666|            2.3475|0.16750653284067746|\n",
       "|2018-12-19|            12|              55.5|            404.775|              1.24|             8.575| 0.5812692341788984|\n",
       "|2019-01-16|            12|            65.395| 344.15000000000003| 8.434999999999999|           10.9725| 0.7294916223823898|\n",
       "|2019-02-07|            21|           89.0425| 0.7000000000000001|              0.61|3.5075000000000003|0.24787322414990423|\n",
       "|2019-02-16|            11|           44.7625|            533.925|5.4799999999999995|11.309999999999999| 0.7498896713561357|\n",
       "|2019-02-21|            12|56.857499999999995|            570.125| 5.242500000000001|            13.105| 0.8564913602169713|\n",
       "|2019-03-12|            14|20.810000000000002|              622.1|            11.015|13.620000000000001| 0.8864995182148631|\n",
       "|2019-03-17|             2|             94.05|               0.65| 5.654218423973333|6.8999999999999995| 0.4741652530350318|\n",
       "|2019-03-28|             5|            66.605|                0.4|            6.3575|              5.74|0.39822482806114856|\n",
       "|2019-04-18|            22|             82.49| 0.4666666666666666|2.9447394746577777| 8.916666666666666| 0.6027532917440291|\n",
       "+----------+--------------+------------------+-------------------+------------------+------------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "eachHourDf.withColumn('delta',4098*eachHourDf['T']/((eachHourDf['T']+237.3)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stima della Et0 con la formula di Valiantzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La radiazione solare viene convertita in MJ/day che è l'unita di misura richiesta per la formula di Valiantzas\n",
    "eachHourDf = eachHourDf.withColumn('Rs',eachHourDf['Rs']*0.0864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachHourDf = eachHourDf.withColumn('Et0',0.0393*eachHourDf['Rs']*((eachHourDf['T']+9.5)**0.5)-0.19*(eachHourDf['Rs']**0.6)\\\n",
    "                                   *(0.73**0.15)+0.048*(eachHourDf['T']+20)\\\n",
    "                                   *(1-(eachHourDf['RH']/100))*(eachHourDf['U2']**0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5823"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachHourDf.select('data','Et0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>features</th><th>Et0</th></tr>\n",
       "<tr><td>[90.6000000000000...</td><td>0.44851653104635747</td></tr>\n",
       "<tr><td>[89.125,0.0388800...</td><td>0.616953535945681</td></tr>\n",
       "<tr><td>[78.815,0.0432,13...</td><td>0.5293128453593872</td></tr>\n",
       "<tr><td>[86.7599999999999...</td><td>0.9534100609984606</td></tr>\n",
       "<tr><td>[94.4425000000000...</td><td>0.04614893374323514</td></tr>\n",
       "<tr><td>[79.975,14.551920...</td><td>3.0112619427221468</td></tr>\n",
       "<tr><td>[76.39,40.5864000...</td><td>7.276769233111957</td></tr>\n",
       "<tr><td>[96.07,0.08208,5....</td><td>0.13601424532850462</td></tr>\n",
       "<tr><td>[82.28,7.52544,9....</td><td>1.7113178838919438</td></tr>\n",
       "<tr><td>[75.84,0.08208000...</td><td>1.0625971517701907</td></tr>\n",
       "<tr><td>[94.8999999999999...</td><td>0.080493411463152</td></tr>\n",
       "<tr><td>[55.5,34.97256,8....</td><td>5.023576945863185</td></tr>\n",
       "<tr><td>[65.395,29.734560...</td><td>6.188786978064014</td></tr>\n",
       "<tr><td>[89.0425,0.060480...</td><td>0.06238046631358229</td></tr>\n",
       "<tr><td>[44.7625,46.13111...</td><td>9.195506638750112</td></tr>\n",
       "<tr><td>[56.8574999999999...</td><td>9.51217746860285</td></tr>\n",
       "<tr><td>[20.8100000000000...</td><td>15.031022038155232</td></tr>\n",
       "<tr><td>[94.05,0.05616,6....</td><td>0.23506202458853537</td></tr>\n",
       "<tr><td>[66.605,0.03456,5...</td><td>1.4872599385063336</td></tr>\n",
       "<tr><td>[82.49,0.04031999...</td><td>0.49801943308190866</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-------------------+\n",
       "|            features|                Et0|\n",
       "+--------------------+-------------------+\n",
       "|[90.6000000000000...|0.44851653104635747|\n",
       "|[89.125,0.0388800...|  0.616953535945681|\n",
       "|[78.815,0.0432,13...| 0.5293128453593872|\n",
       "|[86.7599999999999...| 0.9534100609984606|\n",
       "|[94.4425000000000...|0.04614893374323514|\n",
       "|[79.975,14.551920...| 3.0112619427221468|\n",
       "|[76.39,40.5864000...|  7.276769233111957|\n",
       "|[96.07,0.08208,5....|0.13601424532850462|\n",
       "|[82.28,7.52544,9....| 1.7113178838919438|\n",
       "|[75.84,0.08208000...| 1.0625971517701907|\n",
       "|[94.8999999999999...|  0.080493411463152|\n",
       "|[55.5,34.97256,8....|  5.023576945863185|\n",
       "|[65.395,29.734560...|  6.188786978064014|\n",
       "|[89.0425,0.060480...|0.06238046631358229|\n",
       "|[44.7625,46.13111...|  9.195506638750112|\n",
       "|[56.8574999999999...|   9.51217746860285|\n",
       "|[20.8100000000000...| 15.031022038155232|\n",
       "|[94.05,0.05616,6....|0.23506202458853537|\n",
       "|[66.605,0.03456,5...| 1.4872599385063336|\n",
       "|[82.49,0.04031999...|0.49801943308190866|\n",
       "+--------------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# prendiamo le temperature medie, minime emassime come feature\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['RH','Rs','T','u2'], outputCol = 'features')\n",
    "vstazione_df = vectorAssembler.transform(eachHourDf)\n",
    "vstazione_df = vstazione_df.select(['features', 'Et0'])\n",
    "vstazione_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisione training set e test set\n",
    "splits = vstazione_df.randomSplit([0.8,0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.0468760680033233,0.15365411094286635,0.03157557041568009,0.07971281711748317]\n",
      "Intercept: 3.993764776030423\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Et0', maxIter=20, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.706690\n",
      "r2: 0.958735\n"
     ]
    }
   ],
   "source": [
    "# calcolo del root means square. Siccome stiamo lavorando con dati della radiazione di ordini di grandezza\n",
    "# molto alti, riteniamo accettabile un errore di circa 150\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.726763\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#il modello viene salvato automaticamente su hdfs essendo configurato spark su yarn\n",
    "lr_model.save(\"output/linear_model_evapotranspiration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso di random forest per il modello di regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"Et0\")\n",
    "rf_model = rf.fit(train_df)\n",
    "predictions = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.607774\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Et0\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.save(\"output/random_forest_model_evapotranspiration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso di gradient boost regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(train_df)\n",
    "gbt = GBTRegressor(featuresCol=\"features\",labelCol = 'Et0', maxIter=10)\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "gbt_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbt_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.606528\n",
      "GBTRegressionModel (uid=GBTRegressor_b9a8f013af8c) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Et0\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = gbt_model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model.save('output/gbt_model_evapotranspiration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
