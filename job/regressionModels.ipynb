{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lettura del dataset da MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"pantheon\") \\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.11:2.4.0,org.postgresql:postgresql:42.1.1') \\\n",
    "    .config(\"spark.mongodb.input.uri\",\"mongodb://root:mongodb@mongodb/pantheon.station?authSource=admin\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI='mongodb://mongodb/'\n",
    "def getCollection(sparksession):\n",
    "    df = sparksession.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioneDf = getCollection(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- data_ora: string (nullable = true)\n",
      " |-- id_dato: integer (nullable = true)\n",
      " |-- pioggia_mm: double (nullable = true)\n",
      " |-- pressione_mbar: double (nullable = true)\n",
      " |-- pressione_n_letture: integer (nullable = true)\n",
      " |-- pressione_standard_mbar: double (nullable = true)\n",
      " |-- rad W/mq: double (nullable = true)\n",
      " |-- rad W/mq array: string (nullable = true)\n",
      " |-- rad_n_letture: double (nullable = true)\n",
      " |-- temp1_max: double (nullable = true)\n",
      " |-- temp1_media: double (nullable = true)\n",
      " |-- temp1_min: double (nullable = true)\n",
      " |-- temp1_ur1_n_letture: integer (nullable = true)\n",
      " |-- ur1_max: double (nullable = true)\n",
      " |-- ur1_media: double (nullable = true)\n",
      " |-- ur1_min: double (nullable = true)\n",
      " |-- wind_dir: integer (nullable = true)\n",
      " |-- wind_dir_n_letture: integer (nullable = true)\n",
      " |-- wind_speed_max: double (nullable = true)\n",
      " |-- wind_speed_media: double (nullable = true)\n",
      " |-- wind_speed_n_letture: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stazioneDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifica dei valori nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.id_dato.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.temp1_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.ur1_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22525"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.wind_speed_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20206"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf['rad W/mq'].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf['pressione_mbar'].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riempimento dei valori nulli del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        F.avg(c).alias(c) for c in df.columns if c in include\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioneDf = fill_with_mean(stazioneDf, ['wind_speed_media'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per verificare se le righe sono state riempite correttamente\n",
    "stazioneDf.filter(stazioneDf.wind_speed_media.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.b per esprimere più condizioni in and o or bisogna racchiudere le condizioni tra parentesi tonde\n",
    "from pyspark.sql.functions import hour\n",
    "avgT=stazioneDf.filter((stazioneDf['rad W/mq'].isNotNull())& \\\n",
    "                   ((hour(stazioneDf['data_ora'])<=18)|(hour(stazioneDf['data_ora'])>5)))\n",
    "avgT=stazioneDf.filter((stazioneDf['rad W/mq'].isNotNull())& \\\n",
    "                   ((hour(stazioneDf['data_ora'])<=18)|(hour(stazioneDf['data_ora'])>5)))\n",
    "from pyspark.sql.functions import mean\n",
    "stats=avgT.select([mean('rad W/mq')]).first()\n",
    "avgRadValue= stats.asDict()\n",
    "avgRadValue = avgRadValue['avg(rad W/mq)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.87738295555782"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRadValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,hour\n",
    "\n",
    "stazioneDf = stazioneDf.withColumn('rad W/mq',when((stazioneDf['rad W/mq'].isNull())& ((hour(stazioneDf['data_ora'])>18)|\\\n",
    "                                               (hour(stazioneDf['data_ora'])<=5)),0)\\\n",
    "                               .when((stazioneDf['rad W/mq'].isNull())& ((hour(stazioneDf['data_ora'])<=18)|\\\n",
    "                                               (hour(stazioneDf['data_ora'])>5)),avgRadValue)\\\n",
    "                               .otherwise(stazioneDf['rad W/mq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter((stazioneDf['rad W/mq'].isNull())).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con questa opzione della Spark Session impostata a True, non c'è più bisogno di usare show per vedere i dataframe;\n",
    "# inoltre li posso vedere in modo non sballato\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>rad W/mq</th><th>data_ora</th></tr>\n",
       "<tr><td>104.1</td><td>2018-10-12 14:30:12</td></tr>\n",
       "<tr><td>234.4</td><td>2018-10-12 14:35:11</td></tr>\n",
       "<tr><td>206.6</td><td>2018-10-12 14:45:13</td></tr>\n",
       "<tr><td>75.0</td><td>2018-10-12 14:25:15</td></tr>\n",
       "<tr><td>307.5</td><td>2018-10-12 14:55:13</td></tr>\n",
       "<tr><td>250.3</td><td>2018-10-12 14:50:10</td></tr>\n",
       "<tr><td>360.5</td><td>2018-10-12 15:00:15</td></tr>\n",
       "<tr><td>367.5</td><td>2018-10-12 15:15:13</td></tr>\n",
       "<tr><td>307.5</td><td>2018-10-12 15:05:11</td></tr>\n",
       "<tr><td>388.4</td><td>2018-10-12 15:10:13</td></tr>\n",
       "<tr><td>247.2</td><td>2018-10-12 14:40:13</td></tr>\n",
       "<tr><td>348.9</td><td>2018-10-12 15:20:10</td></tr>\n",
       "<tr><td>340.4</td><td>2018-10-12 15:25:13</td></tr>\n",
       "<tr><td>321.8</td><td>2018-10-12 15:30:15</td></tr>\n",
       "<tr><td>304.4</td><td>2018-10-12 15:35:11</td></tr>\n",
       "<tr><td>294.4</td><td>2018-10-12 15:40:13</td></tr>\n",
       "<tr><td>259.8</td><td>2018-10-12 15:50:12</td></tr>\n",
       "<tr><td>231.3</td><td>2018-10-12 16:00:13</td></tr>\n",
       "<tr><td>128.4</td><td>2018-10-12 16:30:13</td></tr>\n",
       "<tr><td>81.4</td><td>2018-10-12 16:45:11</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+-------------------+\n",
       "|rad W/mq|           data_ora|\n",
       "+--------+-------------------+\n",
       "|   104.1|2018-10-12 14:30:12|\n",
       "|   234.4|2018-10-12 14:35:11|\n",
       "|   206.6|2018-10-12 14:45:13|\n",
       "|    75.0|2018-10-12 14:25:15|\n",
       "|   307.5|2018-10-12 14:55:13|\n",
       "|   250.3|2018-10-12 14:50:10|\n",
       "|   360.5|2018-10-12 15:00:15|\n",
       "|   367.5|2018-10-12 15:15:13|\n",
       "|   307.5|2018-10-12 15:05:11|\n",
       "|   388.4|2018-10-12 15:10:13|\n",
       "|   247.2|2018-10-12 14:40:13|\n",
       "|   348.9|2018-10-12 15:20:10|\n",
       "|   340.4|2018-10-12 15:25:13|\n",
       "|   321.8|2018-10-12 15:30:15|\n",
       "|   304.4|2018-10-12 15:35:11|\n",
       "|   294.4|2018-10-12 15:40:13|\n",
       "|   259.8|2018-10-12 15:50:12|\n",
       "|   231.3|2018-10-12 16:00:13|\n",
       "|   128.4|2018-10-12 16:30:13|\n",
       "|    81.4|2018-10-12 16:45:11|\n",
       "+--------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Controllo per verificare che non siano sovrascritti valori non nulli\n",
    "stazioneDf.select(stazioneDf['rad W/mq'],stazioneDf['data_ora'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggiunta della colonna evapotraspirazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Et0 = 0.0393 Rs * sqrt(T+9.5)-0.19* Rs^0.6 * lat^0.15 +0.048 *(T+20)(1-UMI/100)* u2^0.7\n",
    "LATITUDE=0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,hour,month,dayofyear\n",
    "from pyspark.sql.functions import sum,avg \n",
    "# con questa operazione mi calcolo un dataframe in cui ho per ogni mese la temperatura minima e massima\n",
    "# utile per l'eventuale stima di radiazione solare con la formula di Hargreaves\n",
    "maxMinTemperaterByMonth = stazioneDf.select(to_date(stazioneDf['data_ora']).alias('data')\n",
    "                                            ,dayofyear(to_date(stazioneDf['data_ora'])).alias('giorno'),stazioneDf['temp1_media'])\n",
    "#aggiungo una colonna per fare due diverse aggregazioni sullo stesso valore\n",
    "maxMinTemperaterByMonth = maxMinTemperaterByMonth.withColumn('temp_media_copia',maxMinTemperaterByMonth['temp1_media'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxMinTemperaterByMonth = maxMinTemperaterByMonth\\\n",
    "    .groupBy(month('data')).agg({\"temp1_media\":\"max\",\"temp_media_copia\":\"min\"})\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>month(data)</th><th>max(temp1_media)</th><th>min(temp_media_copia)</th></tr>\n",
       "<tr><td>12</td><td>17.32</td><td>-5.28</td></tr>\n",
       "<tr><td>1</td><td>13.8</td><td>-4.95</td></tr>\n",
       "<tr><td>6</td><td>32.13</td><td>7.27</td></tr>\n",
       "<tr><td>3</td><td>23.02</td><td>-2.48</td></tr>\n",
       "<tr><td>5</td><td>22.63</td><td>1.33</td></tr>\n",
       "<tr><td>4</td><td>26.61</td><td>-0.67</td></tr>\n",
       "<tr><td>10</td><td>26.36</td><td>4.0</td></tr>\n",
       "<tr><td>11</td><td>19.63</td><td>0.36</td></tr>\n",
       "<tr><td>2</td><td>19.94</td><td>-1.84</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+----------------+---------------------+\n",
       "|month(data)|max(temp1_media)|min(temp_media_copia)|\n",
       "+-----------+----------------+---------------------+\n",
       "|         12|           17.32|                -5.28|\n",
       "|          1|            13.8|                -4.95|\n",
       "|          6|           32.13|                 7.27|\n",
       "|          3|           23.02|                -2.48|\n",
       "|          5|           22.63|                 1.33|\n",
       "|          4|           26.61|                -0.67|\n",
       "|         10|           26.36|                  4.0|\n",
       "|         11|           19.63|                 0.36|\n",
       "|          2|           19.94|                -1.84|\n",
       "+-----------+----------------+---------------------+"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxMinTemperaterByMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questa cella serve aggregare i valori per ora, in base da avere valori più realistici di evapotraspirazione\n",
    "eachHourDf = stazioneDf.select([\"data_ora\",\"wind_speed_media\",\"temp1_media\",\"rad W/mq\",\"ur1_media\"])\\\n",
    ".groupBy(to_date(stazioneDf['data_ora']).alias(\"data\"),hour(stazioneDf['data_ora'])).agg({\"wind_speed_media\":\"avg\",\"rad W/mq\":\"avg\",\"ur1_media\":\"avg\"\\\n",
    "                                               ,\"temp1_media\":\"avg\"}).\\\n",
    "    withColumnRenamed('avg(rad W/mq)','Rs').withColumnRenamed('avg(temp1_media)','T')\\\n",
    "    .withColumnRenamed('avg(ur1_media)','RH').withColumnRenamed('avg(wind_speed_media)','u2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachHourDf = eachHourDf.select('data','hour(data_ora)','RH','Rs','u2','T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>data</th><th>hour(data_ora)</th><th>RH</th><th>Rs</th><th>u2</th><th>T</th><th>delta</th></tr>\n",
       "<tr><td>2018-10-17</td><td>18</td><td>90.60000000000001</td><td>0.30000000000000004</td><td>4.27066381798</td><td>17.1825</td><td>1.0872826588951532</td></tr>\n",
       "<tr><td>2018-10-18</td><td>18</td><td>89.125</td><td>0.45</td><td>5.654218423973333</td><td>16.18</td><td>1.0319605003978203</td></tr>\n",
       "<tr><td>2018-10-20</td><td>20</td><td>78.815</td><td>0.5</td><td>1.9810546059933332</td><td>13.4375</td><td>0.8758945993908427</td></tr>\n",
       "<tr><td>2018-10-29</td><td>4</td><td>86.75999999999999</td><td>0.65</td><td>7.670000000000001</td><td>16.83</td><td>1.0679334680651833</td></tr>\n",
       "<tr><td>2018-11-01</td><td>18</td><td>94.44250000000001</td><td>0.44999999999999996</td><td>0.6025</td><td>14.47</td><td>0.9354757316096938</td></tr>\n",
       "<tr><td>2018-11-07</td><td>12</td><td>79.975</td><td>168.425</td><td>5.06</td><td>15.43</td><td>0.9899751321433905</td></tr>\n",
       "<tr><td>2018-11-08</td><td>11</td><td>76.39</td><td>469.75</td><td>2.9975</td><td>16.08</td><td>1.0263921807940448</td></tr>\n",
       "<tr><td>2018-11-11</td><td>3</td><td>96.07000000000001</td><td>0.95</td><td>5.654218423973333</td><td>5.827500000000001</td><td>0.4040043743559911</td></tr>\n",
       "<tr><td>2018-11-27</td><td>11</td><td>82.28</td><td>87.1</td><td>7.5625</td><td>9.447499999999998</td><td>0.6358919576004304</td></tr>\n",
       "<tr><td>2018-12-09</td><td>22</td><td>75.84</td><td>0.9500000000000001</td><td>4.8774999999999995</td><td>10.955</td><td>0.7284308451562326</td></tr>\n",
       "<tr><td>2018-12-10</td><td>7</td><td>94.89999999999999</td><td>2.5749999999999997</td><td>3.217109211986666</td><td>2.3475</td><td>0.16750653284067746</td></tr>\n",
       "<tr><td>2018-12-19</td><td>12</td><td>55.5</td><td>404.775</td><td>1.24</td><td>8.575</td><td>0.5812692341788984</td></tr>\n",
       "<tr><td>2019-01-16</td><td>12</td><td>65.395</td><td>344.15000000000003</td><td>8.434999999999999</td><td>10.9725</td><td>0.7294916223823898</td></tr>\n",
       "<tr><td>2019-02-07</td><td>21</td><td>89.0425</td><td>0.7000000000000001</td><td>0.61</td><td>3.5075000000000003</td><td>0.24787322414990423</td></tr>\n",
       "<tr><td>2019-02-16</td><td>11</td><td>44.7625</td><td>533.925</td><td>5.4799999999999995</td><td>11.31</td><td>0.7498896713561358</td></tr>\n",
       "<tr><td>2019-02-21</td><td>12</td><td>56.857499999999995</td><td>570.125</td><td>5.242500000000001</td><td>13.105</td><td>0.8564913602169713</td></tr>\n",
       "<tr><td>2019-03-12</td><td>14</td><td>20.810000000000002</td><td>622.1</td><td>11.015</td><td>13.620000000000001</td><td>0.8864995182148631</td></tr>\n",
       "<tr><td>2019-03-17</td><td>2</td><td>94.05</td><td>0.65</td><td>5.654218423973333</td><td>6.8999999999999995</td><td>0.4741652530350318</td></tr>\n",
       "<tr><td>2019-03-28</td><td>5</td><td>66.605</td><td>0.4</td><td>6.3575</td><td>5.74</td><td>0.39822482806114856</td></tr>\n",
       "<tr><td>2019-04-18</td><td>22</td><td>82.49</td><td>0.4666666666666666</td><td>2.9447394746577777</td><td>8.916666666666666</td><td>0.6027532917440291</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+--------------+------------------+-------------------+------------------+------------------+-------------------+\n",
       "|      data|hour(data_ora)|                RH|                 Rs|                u2|                 T|              delta|\n",
       "+----------+--------------+------------------+-------------------+------------------+------------------+-------------------+\n",
       "|2018-10-17|            18| 90.60000000000001|0.30000000000000004|     4.27066381798|           17.1825| 1.0872826588951532|\n",
       "|2018-10-18|            18|            89.125|               0.45| 5.654218423973333|             16.18| 1.0319605003978203|\n",
       "|2018-10-20|            20|            78.815|                0.5|1.9810546059933332|           13.4375| 0.8758945993908427|\n",
       "|2018-10-29|             4| 86.75999999999999|               0.65| 7.670000000000001|             16.83| 1.0679334680651833|\n",
       "|2018-11-01|            18| 94.44250000000001|0.44999999999999996|            0.6025|             14.47| 0.9354757316096938|\n",
       "|2018-11-07|            12|            79.975|            168.425|              5.06|             15.43| 0.9899751321433905|\n",
       "|2018-11-08|            11|             76.39|             469.75|            2.9975|             16.08| 1.0263921807940448|\n",
       "|2018-11-11|             3| 96.07000000000001|               0.95| 5.654218423973333| 5.827500000000001| 0.4040043743559911|\n",
       "|2018-11-27|            11|             82.28|               87.1|            7.5625| 9.447499999999998| 0.6358919576004304|\n",
       "|2018-12-09|            22|             75.84| 0.9500000000000001|4.8774999999999995|            10.955| 0.7284308451562326|\n",
       "|2018-12-10|             7| 94.89999999999999| 2.5749999999999997| 3.217109211986666|            2.3475|0.16750653284067746|\n",
       "|2018-12-19|            12|              55.5|            404.775|              1.24|             8.575| 0.5812692341788984|\n",
       "|2019-01-16|            12|            65.395| 344.15000000000003| 8.434999999999999|           10.9725| 0.7294916223823898|\n",
       "|2019-02-07|            21|           89.0425| 0.7000000000000001|              0.61|3.5075000000000003|0.24787322414990423|\n",
       "|2019-02-16|            11|           44.7625|            533.925|5.4799999999999995|             11.31| 0.7498896713561358|\n",
       "|2019-02-21|            12|56.857499999999995|            570.125| 5.242500000000001|            13.105| 0.8564913602169713|\n",
       "|2019-03-12|            14|20.810000000000002|              622.1|            11.015|13.620000000000001| 0.8864995182148631|\n",
       "|2019-03-17|             2|             94.05|               0.65| 5.654218423973333|6.8999999999999995| 0.4741652530350318|\n",
       "|2019-03-28|             5|            66.605|                0.4|            6.3575|              5.74|0.39822482806114856|\n",
       "|2019-04-18|            22|             82.49| 0.4666666666666666|2.9447394746577777| 8.916666666666666| 0.6027532917440291|\n",
       "+----------+--------------+------------------+-------------------+------------------+------------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "eachHourDf.withColumn('delta',4098*eachHourDf['T']/((eachHourDf['T']+237.3)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stima della Et0 con la formula di Valiantzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La radiazione solare viene convertita in MJ/day che è l'unita di misura richiesta per la formula di Valiantzas\n",
    "eachHourDf = eachHourDf.withColumn('Rs',eachHourDf['Rs']*0.0864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachHourDf = eachHourDf.withColumn('Et0',0.0393*eachHourDf['Rs']*((eachHourDf['T']+9.5)**0.5)-0.19*(eachHourDf['Rs']**0.6)\\\n",
    "                                   *(0.73**0.15)+0.048*(eachHourDf['T']+20)\\\n",
    "                                   *(1-(eachHourDf['RH']/100))*(eachHourDf['U2']**0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5824"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachHourDf.select('data','Et0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>features</th><th>Et0</th></tr>\n",
       "<tr><td>[90.6000000000000...</td><td>0.44851653104635747</td></tr>\n",
       "<tr><td>[89.125,0.0388800...</td><td>0.616953535945681</td></tr>\n",
       "<tr><td>[78.815,0.0432,13...</td><td>0.5293128453593872</td></tr>\n",
       "<tr><td>[86.7599999999999...</td><td>0.9534100609984608</td></tr>\n",
       "<tr><td>[94.4425000000000...</td><td>0.04614893374323514</td></tr>\n",
       "<tr><td>[79.975,14.551920...</td><td>3.0112619427221468</td></tr>\n",
       "<tr><td>[76.39,40.5864000...</td><td>7.276769233111957</td></tr>\n",
       "<tr><td>[96.0700000000000...</td><td>0.13601424532850367</td></tr>\n",
       "<tr><td>[82.28,7.52544,9....</td><td>1.7113178838919438</td></tr>\n",
       "<tr><td>[75.84,0.08208000...</td><td>1.0625971517701907</td></tr>\n",
       "<tr><td>[94.8999999999999...</td><td>0.080493411463152</td></tr>\n",
       "<tr><td>[55.5,34.97256,8....</td><td>5.023576945863185</td></tr>\n",
       "<tr><td>[65.395,29.734560...</td><td>6.188786978064014</td></tr>\n",
       "<tr><td>[89.0425,0.060480...</td><td>0.06238046631358229</td></tr>\n",
       "<tr><td>[44.7625,46.13111...</td><td>9.195506638750114</td></tr>\n",
       "<tr><td>[56.8574999999999...</td><td>9.51217746860285</td></tr>\n",
       "<tr><td>[20.8100000000000...</td><td>15.031022038155232</td></tr>\n",
       "<tr><td>[94.05,0.05616,6....</td><td>0.23506202458853537</td></tr>\n",
       "<tr><td>[66.605,0.03456,5...</td><td>1.4872599385063336</td></tr>\n",
       "<tr><td>[82.49,0.04031999...</td><td>0.49801943308190866</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-------------------+\n",
       "|            features|                Et0|\n",
       "+--------------------+-------------------+\n",
       "|[90.6000000000000...|0.44851653104635747|\n",
       "|[89.125,0.0388800...|  0.616953535945681|\n",
       "|[78.815,0.0432,13...| 0.5293128453593872|\n",
       "|[86.7599999999999...| 0.9534100609984608|\n",
       "|[94.4425000000000...|0.04614893374323514|\n",
       "|[79.975,14.551920...| 3.0112619427221468|\n",
       "|[76.39,40.5864000...|  7.276769233111957|\n",
       "|[96.0700000000000...|0.13601424532850367|\n",
       "|[82.28,7.52544,9....| 1.7113178838919438|\n",
       "|[75.84,0.08208000...| 1.0625971517701907|\n",
       "|[94.8999999999999...|  0.080493411463152|\n",
       "|[55.5,34.97256,8....|  5.023576945863185|\n",
       "|[65.395,29.734560...|  6.188786978064014|\n",
       "|[89.0425,0.060480...|0.06238046631358229|\n",
       "|[44.7625,46.13111...|  9.195506638750114|\n",
       "|[56.8574999999999...|   9.51217746860285|\n",
       "|[20.8100000000000...| 15.031022038155232|\n",
       "|[94.05,0.05616,6....|0.23506202458853537|\n",
       "|[66.605,0.03456,5...| 1.4872599385063336|\n",
       "|[82.49,0.04031999...|0.49801943308190866|\n",
       "+--------------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# prendiamo le temperature medie, minime emassime come feature\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['RH','Rs','T','u2'], outputCol = 'features')\n",
    "vstazione_df = vectorAssembler.transform(eachHourDf)\n",
    "vstazione_df = vstazione_df.select(['features', 'Et0'])\n",
    "vstazione_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisione training set e test set\n",
    "splits = vstazione_df.randomSplit([0.8,0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.04696848381153545,0.154029170408527,0.029911748997163274,0.07910808026162172]\n",
      "Intercept: 4.018688765017087\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Et0', maxIter=20, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.706690\n",
      "r2: 0.958735\n"
     ]
    }
   ],
   "source": [
    "# calcolo del root means square. Siccome stiamo lavorando con dati della radiazione di ordini di grandezza\n",
    "# molto alti, riteniamo accettabile un errore di circa 150\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.726763\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#il modello viene salvato automaticamente su hdfs essendo configurato spark su yarn\n",
    "lr_model.save(\"output/linear_model_evapotranspiration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso di random forest per il modello di regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"Et0\")\n",
    "rf_model = rf.fit(train_df)\n",
    "predictions = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.607774\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Et0\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.save(\"output/random_forest_model_evapotranspiration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso di gradient boost regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(train_df)\n",
    "gbt = GBTRegressor(featuresCol=\"features\",labelCol = 'Et0', maxIter=10)\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "gbt_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbt_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.606528\n",
      "GBTRegressionModel (uid=GBTRegressor_b9a8f013af8c) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Et0\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = gbt_model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model.save('output/gbt_model_evapotranspiration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
