{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"pantheon\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_dato=1811253, data_ora=datetime.datetime(2018, 10, 12, 14, 25, 15), temp1_media=25.99, temp1_min=25.93, temp1_max=26.08, ur1_media=56.69, ur1_min=52.56, ur1_max=58.97, temp1_ur1_n_letture=5, pioggia_mm=0.0, rad W/mq=75.0, rad_n_letture=5, wind_dir=6, wind_dir_n_letture=5, wind_speed_media=0.3, wind_speed_max=15.62, wind_speed_n_letture=25, pressione_mbar=1085.94, pressione_standard_mbar=1118.68, pressione_n_letture=5, rad W/mq array='{}')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf = spark.read.format('com.databricks.spark.csv')\\\n",
    ".options(delimiter=';',header='true', inferschema='true').load('input/pantheon20190612-stazione.csv')\n",
    "stazioneDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_dato: integer (nullable = true)\n",
      " |-- data_ora: timestamp (nullable = true)\n",
      " |-- temp1_media: double (nullable = true)\n",
      " |-- temp1_min: double (nullable = true)\n",
      " |-- temp1_max: double (nullable = true)\n",
      " |-- ur1_media: double (nullable = true)\n",
      " |-- ur1_min: double (nullable = true)\n",
      " |-- ur1_max: double (nullable = true)\n",
      " |-- temp1_ur1_n_letture: integer (nullable = true)\n",
      " |-- pioggia_mm: double (nullable = true)\n",
      " |-- rad W/mq: double (nullable = true)\n",
      " |-- rad_n_letture: integer (nullable = true)\n",
      " |-- wind_dir: integer (nullable = true)\n",
      " |-- wind_dir_n_letture: integer (nullable = true)\n",
      " |-- wind_speed_media: double (nullable = true)\n",
      " |-- wind_speed_max: double (nullable = true)\n",
      " |-- wind_speed_n_letture: integer (nullable = true)\n",
      " |-- pressione_mbar: double (nullable = true)\n",
      " |-- pressione_standard_mbar: double (nullable = true)\n",
      " |-- pressione_n_letture: integer (nullable = true)\n",
      " |-- rad W/mq array: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stazioneDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifica dei valori nulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.id_dato.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.temp1_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.ur1_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22525"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.wind_speed_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20206"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf['rad W/mq'].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo valori medi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c in include\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioneDf = fill_with_mean(stazioneDf, ['wind_speed_media'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30802"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter(stazioneDf.wind_speed_media.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.b per esprimere pi√π condizioni in and o or bisogna racchiudere le condizioni tra parentesi\n",
    "from pyspark.sql.functions import hour\n",
    "avgT=stazioneDf.filter((stazioneDf['rad W/mq'].isNotNull())& \\\n",
    "                   ((hour(stazioneDf['data_ora'])<=18)|(hour(stazioneDf['data_ora'])>5)))\n",
    "avgT=stazioneDf.filter((stazioneDf['rad W/mq'].isNotNull())& \\\n",
    "                   ((hour(stazioneDf['data_ora'])<=18)|(hour(stazioneDf['data_ora'])>5)))\n",
    "from pyspark.sql.functions import mean\n",
    "stats=avgT.select([mean('rad W/mq')]).first()\n",
    "avgRadValue= stats.asDict()\n",
    "avgRadValue = avgRadValue['avg(rad W/mq)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.87738295555782"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgRadValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,hour\n",
    "\n",
    "stazioneDf = stazioneDf.withColumn('rad W/mq',when((stazioneDf['rad W/mq'].isNull())& ((hour(stazioneDf['data_ora'])>18)|\\\n",
    "                                               (hour(stazioneDf['data_ora'])<=5)),0)\\\n",
    "                               .when((stazioneDf['rad W/mq'].isNull())& ((hour(stazioneDf['data_ora'])<=18)|\\\n",
    "                                               (hour(stazioneDf['data_ora'])>5)),avgRadValue)\\\n",
    "                               .otherwise(stazioneDf['rad W/mq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stazioneDf.filter((stazioneDf['rad W/mq'].isNull())).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|rad W/mq|           data_ora|\n",
      "+--------+-------------------+\n",
      "|    75.0|2018-10-12 14:25:15|\n",
      "|   104.1|2018-10-12 14:30:12|\n",
      "|   234.4|2018-10-12 14:35:11|\n",
      "|   247.2|2018-10-12 14:40:13|\n",
      "|   206.6|2018-10-12 14:45:13|\n",
      "|   250.3|2018-10-12 14:50:10|\n",
      "|   307.5|2018-10-12 14:55:13|\n",
      "|   360.5|2018-10-12 15:00:15|\n",
      "|   307.5|2018-10-12 15:05:11|\n",
      "|   388.4|2018-10-12 15:10:13|\n",
      "|   367.5|2018-10-12 15:15:13|\n",
      "|   348.9|2018-10-12 15:20:10|\n",
      "|   340.4|2018-10-12 15:25:13|\n",
      "|   321.8|2018-10-12 15:30:15|\n",
      "|   304.4|2018-10-12 15:35:11|\n",
      "|   294.4|2018-10-12 15:40:13|\n",
      "|   277.0|2018-10-12 15:45:13|\n",
      "|   259.8|2018-10-12 15:50:12|\n",
      "|   231.3|2018-10-12 16:00:13|\n",
      "|   195.2|2018-10-12 16:15:10|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stazioneDf.select(stazioneDf['rad W/mq'],stazioneDf['data_ora']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggiunta della colonna evapotraspirazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Et0 = 0.0393 Rs * sqrt(T+9.5)-0.19* Rs^0.6 * lat^0.15 +0.048 *(T+20)(1-UMI/100)* u2^0.7\n",
    "LATITUDE=0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,hour\n",
    "from pyspark.sql.functions import sum,avg \n",
    "# con questa operazione mi calcolo un dataframe in cui ho per ogni giorno la temperatura minima e massima\n",
    "maxMinTemperaterByDay = stazioneDf.select(to_date(stazioneDf['data_ora']).alias('giorno'),stazioneDf['temp1_min'],stazioneDf['temp1_max'])\\\n",
    "    .groupBy('giorno').agg({\"temp1_max\":\"max\",\"temp1_min\":\"min\"})\\\n",
    "    .withColumnRenamed(\"max(temp1_max)\",\"t_max\").withColumnRenamed(\"min(temp1_min)\",\"t_min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eachHourDf = stazioneDf.select([\"data_ora\",\"wind_speed_media\",\"temp1_media\",\"rad W/mq\",\"ur1_media\"])\\\n",
    ".groupBy(to_date(stazioneDf['data_ora']).alias(\"data\"),hour(stazioneDf['data_ora'])).agg({\"wind_speed_media\":\"avg\",\"rad W/mq\":\"avg\",\"ur1_media\":\"avg\"\\\n",
    "                                               ,\"temp1_media\":\"avg\"}).\\\n",
    "    withColumnRenamed('avg(rad W/mq)','Rs').withColumnRenamed('avg(temp1_media)','T')\\\n",
    "    .withColumnRenamed('avg(ur1_media)','RH').withColumnRenamed('avg(wind_speed_media)','U2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eachHourDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------------+-------------------+---------------------+------------------+----------+-----+-----+\n",
      "|      data|hour(data_ora)|   avg(ur1_media)|      avg(rad W/mq)|avg(wind_speed_media)|  avg(temp1_media)|    giorno|t_max|t_min|\n",
      "+----------+--------------+-----------------+-------------------+---------------------+------------------+----------+-----+-----+\n",
      "|2018-10-17|            18|90.60000000000001|                0.3|        4.27066381798|           17.1825|2018-10-17|23.01|12.09|\n",
      "|2018-10-18|            18|           89.125|               0.45|    5.654218423973333|             16.18|2018-10-18|27.01| 12.3|\n",
      "|2018-10-20|            20|           78.815|                0.5|   1.9810546059933332|           13.4375|2018-10-20|25.51| 8.98|\n",
      "|2018-10-29|             4|86.75999999999999|               0.65|                 7.67|16.830000000000002|2018-10-29|21.19| 9.98|\n",
      "|2018-11-01|            18|94.44250000000001|0.44999999999999996|               0.6025|             14.47|2018-11-01|16.07|11.63|\n",
      "|2018-11-07|            12|           79.975|            168.425|                 5.06|             15.43|2018-11-07|18.24| 5.66|\n",
      "|2018-11-08|            11|            76.39|             469.75|               2.9975|             16.08|2018-11-08|19.08| 6.17|\n",
      "|2018-11-11|             3|            96.07|               0.95|    5.654218423973333|            5.8275|2018-11-11|19.62| 3.99|\n",
      "|2018-11-27|            11|            82.28|               87.1|               7.5625| 9.447499999999998|2018-11-27|10.89| 7.14|\n",
      "|2018-12-09|            22|            75.84| 0.9500000000000001|   4.8774999999999995|            10.955|2018-12-09|15.62|-0.49|\n",
      "+----------+--------------+-----------------+-------------------+---------------------+------------------+----------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join tra tabella oraria e tabella delle temperature minime e massime, per avere per ogni record della prima\n",
    "# tabella la temperatura giornaliera massima e minima, utile per il calcolo di Et0\n",
    "eachHourDf.join(maxMinTemperaterByDay,eachHourDf.data==maxMinTemperaterByDay.giorno,'inner')\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "eachHourDf = eachHourDf.select(col(\"avg(ur1_media)\").alias(\"RH\"),col(\"avg(temp1_media)\").alias(\"T\"),\n",
    "                              col(\"avg(wind_speed_media)\").alias(\"u2\"),col())\n",
    "eachHourDf.show()\n",
    "eachHourDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"avg(temp1_media)\" among (data, hour(data_ora), RH, Rs, U2, T);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/sparkenv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sparkenv/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o730.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"avg(temp1_media)\" among (data, hour(data_ora), RH, Rs, U2, T);\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:222)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1268)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1235)\n\tat sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b396baed12a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meachHourDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4098\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meachHourDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg(temp1_media)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meachHourDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg(temp1_media)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m237.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/sparkenv/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \"\"\"\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sparkenv/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sparkenv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"avg(temp1_media)\" among (data, hour(data_ora), RH, Rs, U2, T);'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eachHourDf.withColumn('delta',4098*eachHourDf['avg(temp1_media)']/((eachHourDf['avg(temp1_media)']+237.3)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stima della Et0 con la formula di Valiantzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachHourDf = eachHourDf.withColumn('Rs',eachHourDf['Rs']*0.0864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachHourDf = eachHourDf.withColumn('Et0',0.0393*eachHourDf['Rs']*((eachHourDf['T']+9.5)**0.5)-0.19*(eachHourDf['Rs']**0.6)\\\n",
    "                                   *(0.73**0.15)+0.048*(eachHourDf['T']+20)\\\n",
    "                                   *(1-(eachHourDf['RH']/100))*(eachHourDf['U2']**0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5823"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachHourDf.select('data','Et0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|                Et0|\n",
      "+--------------------+-------------------+\n",
      "|[90.6000000000000...|0.44851653104635747|\n",
      "|[89.125,0.0388800...|  0.616953535945681|\n",
      "|[78.815,0.0432,13...| 0.5293128453593872|\n",
      "+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# prendiamo le temperature medie, minime emassime come feature\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['RH','Rs','T','U2'], outputCol = 'features')\n",
    "vstazione_df = vectorAssembler.transform(eachHourDf)\n",
    "vstazione_df = vstazione_df.select(['features', 'Et0'])\n",
    "vstazione_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisione training set e test set\n",
    "\n",
    "splits = vstazione_df.randomSplit([0.8,0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.04690388917603679,0.15422079514605752,0.02891093063249502,0.0796713754970911]\n",
      "Intercept: 4.016408716508319\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Et0', maxIter=20, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.706690\n",
      "r2: 0.958735\n"
     ]
    }
   ],
   "source": [
    "# calcolo del root means square. Siccome stiamo lavorando con dati della radiazione di ordini di grandezza\n",
    "# molto alti, riteniamo accettabile un errore di circa 150\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.725374\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso di random forest per il modello di regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"Et0\")\n",
    "rf_model = rf.fit(train_df)\n",
    "predictions = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.578568\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Et0\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stima dei valori di radiazione mancante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioneDfWithNoNull=stazioneDf.filter(stazioneDf['rad W/mq'].isNotNull())\n",
    "stazioneDfWithNoNull.select(stazioneDf['rad W/mq'].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# prendiamo le temperature medie, minime emassime come feature\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['temp1_media','temp1_min','temp1_max'], outputCol = 'features')\n",
    "vstazione_df = vectorAssembler.transform(stazioneDfWithNoNull)\n",
    "vstazione_df = vstazione_df.select(['features', 'rad W/mq'])\n",
    "vstazione_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisione training set e test set\n",
    "\n",
    "splits = vstazione_df.randomSplit([0.8,0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='rad W/mq', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo del root means square. Siccome stiamo lavorando con dati della radiazione di ordini di grandezza\n",
    "# molto alti, riteniamo accettabile un errore di circa 150\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"rad W/mq\",\"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"MV\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'rad W/mq')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"rad W/mq\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stazioneDf.select('rad W/mq').rdd.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingglrSummary = glr_model.summary\n",
    "trainingglrSummary.coefficientStandardErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
